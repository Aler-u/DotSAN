{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.layout import LAParams\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from unidecode import unidecode\n",
    "from itertools import compress\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates a series of metrics for each output from the parsing of a single PDF. These metrics help to establish the performance  of each parser and compare among them. \n",
    "\n",
    "Different metrics have been selected based on the available information and the associated usefulness. \n",
    "The length of the title and poster strings are useful to determine the number of notable outliers that would indicate misclassified items, such as remarkably short titles or extremely lengthy authors. Thus we include both a histogram and the 90% interval of values. Using the name database shown next we can compare the histogram of names length to the histogram of the authors names.  \n",
    "\n",
    "Moreover, for the poster topics, we include the frequency count of each poster topic detected. \n",
    "\n",
    "Lastly, we examine the proportion of non-missing values to the number of pages sampled. Under the assumption that, in the current files, each page contains but one poster this metric should reflect the number of poster information \"correctly\" extracted.\n",
    "The metrics include:\n",
    "\n",
    "* Histogram of poster title length (in characters)\n",
    "* Histogram of author name lenght calculated (number of words separated by whitespace)\n",
    "* 95-5 percentiles and count of poster title and author name length\n",
    "* Values count for the poster topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_performance(dataset, no_pages = None):\n",
    "    title_len = dataset['titulo'].dropna().str.len().values\n",
    "    authors = dataset['autor'].dropna()\n",
    "    authors = authors.apply(lambda x: unidecode(x))\n",
    "    authors = authors.apply(lambda x: re.compile(r'[a-zA-Z\\s]*').search(x).group())\n",
    "    authors_len = authors.str.split(' ').apply(len).values\n",
    "    print('\\n','Authors names length','\\n')\n",
    "    plt.hist(authors_len)\n",
    "    plt.show()\n",
    "    print('\\n','Poster title length','\\n')\n",
    "    plt.hist(title_len)\n",
    "    plt.show()\n",
    "    author_95 = np.percentile(authors_len, [5,95])\n",
    "    title_95 = np.percentile(title_len, [5,95])\n",
    "    print('-'*50)\n",
    "    print('Authors')\n",
    "    print('5-95 percentiles:','\\n',\n",
    "          '5th: {percentil5}'.format(percentil5 = author_95[0]),' - ',\n",
    "          '95th: {percentil95}'.format(percentil95 = author_95[1]),'\\n',\n",
    "         '5th count: {cuenta5}'.format(cuenta5 = np.sum(authors_len < author_95[0])),' - ',\n",
    "         '95th count: {cuenta95}'.format(cuenta95 = np.sum(authors_len > author_95[1])),\n",
    "        '\\n',\n",
    "         '-'*50,\n",
    "        '\\n',\n",
    "          'Titles',\n",
    "        '\\n',\n",
    "          '5-95 percentiles:',\n",
    "        '\\n',\n",
    "          '5th: {percentil5}'.format(percentil5 = title_95[0]),' - ',\n",
    "          '95th: {percentil95}'.format(percentil95 = title_95[1]),'\\n',\n",
    "         '5th count: {cuenta5}'.format(cuenta5 = np.sum(title_len < title_95[0])),' - ',\n",
    "         '95th count: {cuenta95}'.format(cuenta95 = np.sum(title_len > title_95[1])),\n",
    "        '\\n',\n",
    "         '-'*50,\n",
    "        '\\n',\n",
    "            dataset['tema'].value_counts()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Authors names length \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Poster title length \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN9klEQVR4nO3dcaid913H8ffHZBbcprYmLaGN3mxEsf3DroQqzI1JZW1XXTqlI0UkYCEKHayoYGrB9Z9Ap2z+ZTcyVhakW1fZSgOd2hKGxT9cd1PTLlkam63ZmiUmWSu0olSTff3jPpknt+fce3PPPXnO/fX9gsN5nu/5nfN88zsnn/vc55zz3FQVkqS2/ETfDUiSVp7hLkkNMtwlqUGGuyQ1yHCXpAat7bsBgHXr1tXMzEzfbUjSqrJ///4fVtX6YbdNRbjPzMwwOzvbdxuStKok+d6o2zwsI0kNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDZqKb6hKi5nZ+URv2z72wG29bVtaLvfcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGLhnuSjUm+nuRwkkNJPt7Vr0jyVJIXu+vLB+5zb5KjSY4kuXmS/wBJ0pstZc/9LPAnVfXLwK8Bdye5FtgJ7KuqzcC+bp3utm3AdcAtwINJ1kyieUnScIuGe1WdrKpnu+XXgcPA1cBWYE83bA9we7e8FXikqt6oqpeAo8CNK9y3JGkBF3XMPckM8B7gG8BVVXUS5n4AAFd2w64GXh642/GuNv+xdiSZTTJ75syZZbQuSRplyeGe5B3AV4B7quq1hYYOqdWbClW7q2pLVW1Zv379UtuQJC3BksI9yduYC/aHq+qrXflUkg3d7RuA0139OLBx4O7XACdWpl1J0lIs5dMyAT4PHK6qTw/ctBfY3i1vBx4fqG9LclmSTcBm4JmVa1mStJi1SxjzXuD3gW8lOdDV/hx4AHg0yV3A94E7AKrqUJJHgW8z90mbu6vq3Eo3LkkabdFwr6p/ZvhxdICbRtxnF7BrjL4kSWPwG6qS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGre27Aa0uMzuf6LsFSUtguEuL6OsH2rEHbutlu2qDh2UkqUGGuyQ1yHCXpAZ5zH0V8k1NSYtxz12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYtGu5JHkpyOsnBgdr9SX6Q5EB3+dDAbfcmOZrkSJKbJ9W4JGm0pey5fwG4ZUj9r6vq+u7yNYAk1wLbgOu6+zyYZM1KNStJWppFw72qngZeXeLjbQUeqao3quol4Chw4xj9SZKWYZxj7h9L8nx32ObyrnY18PLAmONdTZJ0CS033D8DvBu4HjgJfKqrZ8jYGvYASXYkmU0ye+bMmWW2IUkaZlnhXlWnqupcVf0I+Bz/f+jlOLBxYOg1wIkRj7G7qrZU1Zb169cvpw1J0gjLCvckGwZWPwKc/yTNXmBbksuSbAI2A8+M16Ik6WItesrfJF8CPgCsS3Ic+ATwgSTXM3fI5RjwhwBVdSjJo8C3gbPA3VV1biKdS5JGWjTcq+rOIeXPLzB+F7BrnKYkSePxG6qS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGLhnuSh5KcTnJwoHZFkqeSvNhdXz5w271JjiY5kuTmSTUuSRptKXvuXwBumVfbCeyrqs3Avm6dJNcC24Druvs8mGTNinUrSVqSRcO9qp4GXp1X3grs6Zb3ALcP1B+pqjeq6iXgKHDjyrQqSVqq5R5zv6qqTgJ011d29auBlwfGHe9qb5JkR5LZJLNnzpxZZhuSpGFW+g3VDKnVsIFVtbuqtlTVlvXr169wG5L01rbccD+VZANAd326qx8HNg6MuwY4sfz2JEnLsdxw3wts75a3A48P1LcluSzJJmAz8Mx4LUqSLtbaxQYk+RLwAWBdkuPAJ4AHgEeT3AV8H7gDoKoOJXkU+DZwFri7qs5NqHdJ0giLhntV3TnipptGjN8F7BqnKUnSePyGqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNWtt3A5KGm9n5RN8tXHLHHrit7xaa4Z67JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg8Y6K2SSY8DrwDngbFVtSXIF8GVgBjgGfLSq/mO8NiVJF2Ml9tx/o6qur6ot3fpOYF9VbQb2deuSpEtoEodltgJ7uuU9wO0T2IYkaQHjhnsBTybZn2RHV7uqqk4CdNdXDrtjkh1JZpPMnjlzZsw2JEmDxv1LTO+tqhNJrgSeSvLCUu9YVbuB3QBbtmypMfuQJA0Ya8+9qk5016eBx4AbgVNJNgB016fHbVKSdHGWHe5J3p7kneeXgQ8CB4G9wPZu2Hbg8XGblCRdnHEOy1wFPJbk/ON8sar+Ick3gUeT3AV8H7hj/DYlSRdj2eFeVd8FfmVI/RXgpnGakiSNZ9w3VN/SZnY+0XcLkjSUpx+QpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3ylL+SpkZfp9E+9sBtvWx3ktxzl6QGGe6S1CDDXZIaZLhLUoMMd0lqkJ+WkfSW1+cfu5/UJ3Xcc5ekBhnuktQgw12SGmS4S1KDDHdJalATn5bp851uSZpG7rlLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatDEwj3JLUmOJDmaZOektiNJerOJhHuSNcDfALcC1wJ3Jrl2EtuSJL3ZpPbcbwSOVtV3q+p/gEeArRPaliRpnkmdFfJq4OWB9ePArw4OSLID2NGt/meSI2Nucx3wwzEf41JYDX3a48pZDX3a48q56D7zybG29wujbphUuGdIrS5YqdoN7F6xDSazVbVlpR5vUlZDn/a4clZDn/a4cqapz0kdljkObBxYvwY4MaFtSZLmmVS4fxPYnGRTkp8EtgF7J7QtSdI8EzksU1Vnk3wM+EdgDfBQVR2axLYGrNghnglbDX3a48pZDX3a48qZmj5TVYuPkiStKn5DVZIaZLhLUoNWZbgn2Zjk60kOJzmU5ONd/f4kP0hyoLt8qOc+jyX5VtfLbFe7IslTSV7sri/vsb9fGpirA0leS3LPNMxjkoeSnE5ycKA2cu6S3Nud6uJIkpt77PGvkryQ5PkkjyX52a4+k+S/B+b0s5eixwX6HPkcT9Fcfnmgv2NJDnT1XuZygdyZqtflj1XVqrsAG4AbuuV3Av/G3GkO7gf+tO/+Bvo8BqybV/tLYGe3vBP4ZN99dr2sAf6duS9F9D6PwPuBG4CDi81d99w/B1wGbAK+A6zpqccPAmu75U8O9DgzOG4K5nLoczxNcznv9k8Bf9HnXC6QO1P1ujx/WZV77lV1sqqe7ZZfBw4z963Y1WArsKdb3gPc3l8rF7gJ+E5Vfa/vRgCq6mng1XnlUXO3FXikqt6oqpeAo8ydAuOS91hVT1bV2W71X5j7jkevRszlKFMzl+clCfBR4EuT7mMhC+TOVL0uz1uV4T4oyQzwHuAbXelj3a/ED/V5yKNTwJNJ9nenWwC4qqpOwtyLBbiyt+4utI0L//NM0zyeN2ruhp3uYhp+2P8B8PcD65uS/GuSf0ryvr6aGjDsOZ7GuXwfcKqqXhyo9TqX83JnKl+Xqzrck7wD+ApwT1W9BnwGeDdwPXCSuV/l+vTeqrqBubNj3p3k/T33M1T3RbMPA3/XlaZtHhez6OkuLrUk9wFngYe70kng56vqPcAfA19M8tN99cfo53jq5hK4kwt3PHqdyyG5M3LokNolm8tVG+5J3sbcBD9cVV8FqKpTVXWuqn4EfI5L+CvQMFV1ors+DTzW9XMqyQaA7vp0fx3+2K3As1V1CqZvHgeMmrupOt1Fku3AbwG/V93B1+5X81e65f3MHX/9xb56XOA5nra5XAv8DvDl87U+53JY7jClr8tVGe7dMbjPA4er6tMD9Q0Dwz4CHJx/30slyduTvPP8MnNvtB1k7jQM27th24HH++nwAhfsGU3TPM4zau72AtuSXJZkE7AZeKaH/khyC/BnwIer6r8G6usz93cOSPKursfv9tFj18Oo53hq5rLzm8ALVXX8fKGvuRyVO0zr6/JSv+O8Ehfg15n79eZ54EB3+RDwt8C3uvpeYEOPPb6LuXfKnwMOAfd19Z8D9gEvdtdX9DyXPwW8AvzMQK33eWTuh81J4H+Z2wO6a6G5A+5jbg/uCHBrjz0eZe446/nX5We7sb/bvQ6eA54FfrvnuRz5HE/LXHb1LwB/NG9sL3O5QO5M1evy/MXTD0hSg1blYRlJ0sIMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSg/wNluK1mgOZukgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Authors\n",
      "5-95 percentiles: \n",
      " 5th: 2.0  -  95th: 5.0 \n",
      " 5th count: 16  -  95th count: 24 \n",
      " -------------------------------------------------- \n",
      " Titles \n",
      " 5-95 percentiles: \n",
      " 5th: 55.0  -  95th: 175.5 \n",
      " 5th count: 48  -  95th count: 49 \n",
      " -------------------------------------------------- \n",
      " Cellular and Molecular Neurobiology      379\n",
      "Cognition, Behavior, and Memory          252\n",
      "Neurochemistry and Neuropharmacology     122\n",
      "Neural Circuit Physiology                 87\n",
      "Chronobiology                             71\n",
      "Motor Systems                             31\n",
      "Computational Neuroscience                23\n",
      "Cellular and Molecular neurobiology        1\n",
      "Name: tema, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Example of the function use and output\n",
    "parser_performance(pd.read_csv('../SAN_csv/SAN_2015.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset containing all the names of the science and technology staff according to data from the [SICTYAR](https://datos.gob.ar/dataset/mincyt-personal-ciencia-tecnologia). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "personas = pd.read_csv('personas.csv', sep = ';', low_memory = False,\n",
    "                      encoding='UTF-8')\n",
    "personas_na = personas[personas['nombre'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step:\n",
    "\n",
    "* Clean the name format using unidecode\n",
    "* Transform all names to lowercase\n",
    "* Split the string by each word and count the frequency of each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nombres_personas = personas_na['nombre'].map(unidecode).str.lower().str.split(expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the results to leave only words with a frequency higher than 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nombres_personas = nombres_personas[nombres_personas >= 150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list can be used as a target keyword container to detect the presence of author names in a certain parsed element. Although it is only used in the 2012 function we add the whole process inside the wrapper function before the iteration over each dictionary key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, there will be a code breakdown for each pdf file parsing function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAN 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes one argument, the parsed pdf string.\n",
    "\n",
    "The pdf file for this year contains the poster topic as the first element when the string is divided by a newline. \n",
    "\n",
    "The title is found by using a regex approach where each string of the list that results from splitting the pdf page by two consecutive newlines is evaluated to a boolean if it doesn't return a Match object for the title regex. \n",
    "\n",
    "Finally the authors are given by the second element of the list that results from splitting the pdf page by two non-consecutive newlines. \n",
    "\n",
    "It returns a pandas data frame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def san_2019_parser(poster_string):\n",
    "    tema = poster_string.split('\\n')[0]\n",
    "    titulo = compress(\n",
    "            poster_string.split('\\n\\n'), \n",
    "            [titulo_regex.search(elements) != None for elements in poster_string.split('\\n\\n')]\n",
    "    )\n",
    "    autores = poster_string.split('\\n \\n')[2]\n",
    "    return pd.DataFrame(\n",
    "                {'autor': autores.split(','),\n",
    "                'tema': tema,\n",
    "                'poster': titulo}\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAN 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes one argument, the parsed pdf string.\n",
    "\n",
    "The pdf file for this year exhibits the same structure as the file from 2019, therefore it follows the same parsing procedure with the exception of the authors which are extracted via the index of the next element after the title when splitting the pdf page by two consecutive newlines and then selecting the first element of the resulting string after splitting it by two non-consecutive newlines.\n",
    "\n",
    "It returns a pandas data frame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def san_2018_parser(poster_string):\n",
    "    tema = poster_string.split('\\n\\n')[0]\n",
    "    titulo = compress(\n",
    "            poster_string.split('\\n\\n'), \n",
    "            [titulo_regex.search(elements) != None for elements in poster_string.split('\\n\\n')]\n",
    "    )\n",
    "    autores = poster_string.split('\\n\\n')[[titulo_regex.search(elements) != None for elements in poster_string.split('\\n\\n')]].split('\\n \\n')[0]\n",
    "    return pd.DataFrame(\n",
    "                {'autor': autores.split(','),\n",
    "                'tema': tema,\n",
    "                'poster': titulo}\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAN 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsing function for this year's pdf takes two arguments. Both arguments are strings obtained from the same pdf page with different parsing methods, pdfminer and PyPDF2, respectively.\n",
    "\n",
    "The poster topic is obtained by indexing the second element after splitting the string by two whitespaces. The poster title is the following element to the topic and the authors are obtained by indexing the third element after splitting the pdf string by two non-consecutive newlines.\n",
    "\n",
    "It returns a pandas data frame object with the same structure as the template data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def san_2017_parser(poster_str, poster_str2):\n",
    "    #Lista con los datos del poster \n",
    "    poster_spliteado = poster_str.split('  ')\n",
    "    #Tema del poster\n",
    "    tema = poster_spliteado[1].strip()\n",
    "    #Titulo del poster\n",
    "    titulo = poster_spliteado[2]\n",
    "    #Lista con los autores del poster\n",
    "    autores = poster_str2.split('\\n \\n')[2].split(',')\n",
    "    #\n",
    "    return pd.DataFrame(\n",
    "                    {'autor': autores,\n",
    "                    'poster': titulo,\n",
    "                    'tema': tema}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itero por cada pagina dentro del rango definido anteriormente y para cada una extraigo los datos usando la libreria pdfminer y PyPDF2 para entregarle las strings como argumento a la funcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAN 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Annual Meeting, under the auspices of FALAN Buenos Aires Meeting, several events were held:\n",
    "* The¨XXXIX Reunião Anual da la Sociedade Brasileira de Neurociências e Comportamento¨\n",
    "* The ¨XII Reunión Anual de la Sociedad Chilena de Neurociencias¨\n",
    "* The ¨XV Jornadas de la Sociedad de Neurociencias del Uruguay¨\n",
    "* The ¨XXXI Congreso Anual de la Sociedad Argentina de Investigación en Neurociencias\"\n",
    "\n",
    "This means that there is a huge mixture of authors (non-members of the SAN) in the poster session as well as an increase number of posters, in addittion to a problematic layout in the PDF file.\n",
    "\n",
    "For these reasons the Annual Meeting of 2016 was excluded from the parsing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAN 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes one argument, a parsed pdf string. \n",
    "\n",
    "The poster topic is extracted by indexing the second element from the list created by splitting the parsed string by two non-consecutive newlines. The poster title is the following element.\n",
    "\n",
    "The poster authors and afiliations are the fourth element by indexing the list created by splitting the parsed string by two non-consecutive newlines. The result is then splited by a newline after which two new variables are assigned corresponding to the first and second element with error handling for the second element that prompts a return statement thus halting the code at that point.  \n",
    "In addition, a conditional statement evaluates if the length of the string from the first element of the authors list is higher than 85 which would indicate the presence of a great number of authors and increases the likelihood of the next element also containing author names.  \n",
    "Therefore, when the conditional statement evaluates to True, the second element is examined for the presence of 3 keywords to determine if it corresponds to more authors or to affiliations. If some of the keywords returns a match then only the first element is used and is transformed in the same way as if the length of the first element was less than 85. \n",
    "When the conditional statement evaluates to False, the second element and the first element are combined and transformed to create the final authors list.\n",
    "\n",
    "It returns a pandas data frame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def san_2015_parser(poster_string):\n",
    "    tema = poster_string.split('\\n \\n')[1]\n",
    "    titulo = poster_string.split('\\n \\n')[2]\n",
    "    titulo = ''.join(titulo.splitlines())\n",
    "    autores = poster_string.split('\\n \\n')[3]\n",
    "    elementos = autores.split('\\n')\n",
    "    primer_elemento = elementos[0]\n",
    "    try:\n",
    "        segundo_elemento = elementos[1]\n",
    "    except:\n",
    "        print(f'Error en pagina {i + 1}')\n",
    "        return()\n",
    "    if len(primer_elemento) > 85:\n",
    "        if 'Laboratorio' in segundo_elemento or 'Instituto' in segundo_elemento or 'CONICET' in segundo_elemento:\n",
    "            autores = primer_elemento.split(',')\n",
    "        else:            \n",
    "            autores = ''.join(autores.split('\\n')[0:2]).split(',')\n",
    "    else:\n",
    "        autores = primer_elemento.split(',')\n",
    "    return pd.DataFrame(\n",
    "                        {'autor': [autor.strip() for autor in autores],\n",
    "                        'poster': titulo,\n",
    "                        'tema': tema}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAN 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No element in the pdf structure allow us to distinguish between the elements containing the title, authors or affiliations. \n",
    "\n",
    "As a consequence, we first iterate over all the posters containing pages from the pdf and tokenize the strings of the elements that are most likely to hold affiliation data. Afterward, we calculate the frequency of each term and manually select relevant non-redundant words from the most frequent ones to create an affiliations keywords list.  \n",
    "Since this process needs to be performed only once and the keyword selection is done manually, to avoid including non-related keywords, the code to generate the keywords list is not included in the parsing function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creo una serie de pandas con los tokens identificados del tercer y cuarto elemento de cada pagina\n",
    "res = pd.Series()\n",
    "tk = WordPunctTokenizer()\n",
    "for i in range(43,192):\n",
    "    tmp_page = extract_text('SAN_2014.pdf', page_numbers=[i]).split('\\n')\n",
    "    word_list = tk.tokenize(tmp_page[3] + tmp_page[4]) \n",
    "    res = res.append(pd.Series(word_list), True)\n",
    "#Ordeno los elementos segun frecuencia\n",
    "res.value_counts()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most relevant keywords to identify affiliations are:\n",
    "* CONICET\n",
    "* Instituto\n",
    "* Laboratorio\n",
    "* Facultad\n",
    "* Argentina\n",
    "* Universidad\n",
    "* Departamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filiaciones_keywords_2014 = ['CONICET', 'Instituto', 'Laboratorio', 'Facultad', 'Argentina', 'Universidad', 'Departamento']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes one argument\n",
    "\n",
    "_poster_string_ = expects a string resulting from the parsing of a page from the pdf file  \n",
    "\n",
    "\n",
    "The poster topic appears only once, in the first poster of that topic. Thus, the poster topic must remain the same for all the posters following the first until another poster topic appears. Therefore we create a global variable inside the function for the poster topic but assing it a different name reflecting the unique nature of this variable for this pdf file and, primarily, to avoid collisions with the (same name) variable in the local environment of the other functions.  \n",
    "If the first element is considered a poster topic the global poster variable is assigned to that element, otherwise the first element is considered the poster title. Therefore we add the global poster topic variable to the wrapper function.\n",
    "\n",
    "To distinguish between the authors and the title we employ a regex approach by taking advantage of the author's structure. We use two regex patterns, one for the authors and one for the title. The regex title is the same we use in the 2018 or 2019 files so we only need to generate the regex pattern for the authors and since this regex is only used in this parser we include it as a local variable inside the function.  \n",
    "Therefore, if the regex returns a list with a length lower than 2 we may conclude that it is not the element that contains the authors.  \n",
    "However this method cannot accurately detect posters with only one author although, ancedotally, it is a rare ocurrence within this file. \n",
    "\n",
    "\n",
    "In order to distinguish between authors and affiliations we employ a keyword approach with fuzzy string matching. As a consequence if the element under examination contains a high score it will be considered as an affiliations element.\n",
    "\n",
    "Lastly, to distinguish pages were the first element is the posters topic insted of the title we again employ a regex approach to examine the existence of the letter P followed by some digit and the characters \".-\" that preced every poster title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def san_2014_parser(poster_string, tema_2014 = 'Cellular and Molecular Neurobiology'):\n",
    "    autores_regex = re.compile(r'(\\S*\\s\\S*),')\n",
    "    res_df = pd.DataFrame({'autor': [], 'poster': [], 'tema':[]})\n",
    "    #Separo la string segun newlines\n",
    "    string_separada = poster_string.split('\\n')\n",
    "    #Chequeo si el primer elemento es el tema del poster o el titulo\n",
    "    if titulo_regex.search(string_separada[0]) == None:\n",
    "        #Si es True considero que el primer elemento es el tema del poster y no el titulo\n",
    "        global tema_2014\n",
    "        tema_2014 = string_separada[0]\n",
    "        titulo = string_separada[2]\n",
    "        indice = 2\n",
    "    else:\n",
    "        #Si es False considero que el primer elemento es el titulo del poster\n",
    "        titulo = string_separada[0]\n",
    "        indice = 0\n",
    "    #Chequeo si el proximo elemento es parte del titulo segun la longitud del resultado del regex\n",
    "    if len(autores_regex.findall(string_separada[indice + 1])) < 2:\n",
    "        #Si es True considero que es parte del titulo por lo que se lo agrego a la variable\n",
    "        titulo = titulo + ' ' + string_separada[indice + 1]\n",
    "        autores = string_separada[indice + 2]\n",
    "        indice = indice + 2\n",
    "    else:\n",
    "        #Si es False considero que no es parte del titulo sino que son los autores\n",
    "        autores = string_separada[indice + 1]\n",
    "        indice = indice + 1\n",
    "    #Chequeo si el proximo elemento es parte de los autores o de las filiaciones\n",
    "    if any(afil_key in string_separada[indice + 1] for afil_key in filiaciones_keywords_2014) or process.extractOne(query = string_separada[indice + 2], choices = filiaciones_keywords_2014, scorer = fuzz.partial_ratio)[1] > 85:\n",
    "        #Si es True entonces el elemento son las filiaciones y no es de interes\n",
    "        pass\n",
    "    else:\n",
    "        #Si es False entonces el elemento aun contiene autores y tengo que agregarlo a la lista anterior\n",
    "        autores = autores + string_separada[indice + 2]\n",
    "    #Itero por cada autor usando las comas como separador\n",
    "    for aut in autores.split(','):\n",
    "        tmp_df = pd.DataFrame(\n",
    "            {'autor': [aut], 'poster': [titulo], 'tema': [tema_2014]}\n",
    "        )\n",
    "        res_df = res_df.append(tmp_df)\n",
    "    return(res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAN 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've done in the SAN 2014 parsing function, we generate the affiliations keywords list after tokenizing the strings from the third and fourth element of all the pages that contain poster information. Again, we manually select relevant keywords to construct the list, and we don't include the code in the parsing function following the same reasons as before. Although in this case, we search the 50 most frequent tokens instead of the 30 most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creo una serie de pandas con los tokens identificados del tercer y cuarto elemento de cada pagina\n",
    "res = pd.Series()\n",
    "for i in tqdm(range(113,341)):\n",
    "    tmp_page = extract_text('SAN_2013.pdf', page_numbers=[i]).split('\\n \\n')[2].split('\\n')\n",
    "    try:\n",
    "        word_list = tk.tokenize(tmp_page[2] + tmp_page[3] + tmp_page[4] + tmp_page[5]) \n",
    "        res = res.append(pd.Series(word_list), True)\n",
    "    except:\n",
    "        continue\n",
    "res.value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filiaciones_keywords_2013 = {'CONICET', 'Facultad', 'Instituto', 'Universidad', 'University', 'Laboratorio', 'Laboratory', 'Argentina', 'UBA', 'Departamento'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes one argument, a string resulting from the parsing of a page from the pdf file\n",
    "\n",
    "The poster theme is given by the first element of the parsed PDF file, splited by a newline. \n",
    "\n",
    "The title may be given by different elements with different splitting methods. To recognize a parsing falilure we examine the element's length and if it exceeds 1000 characters we asume the element contains more than the title. We first try the second element of the parsed PDF file, splited by two non-consecutive newlines. Upon failure for the above criteria we try the second element splitted by two consecutive newlines and, ultimately, we use the poster session element to locate the title. \n",
    "\n",
    "There is no constant division between the authors and the title, sometimes it is given by one newline, two consecutive newlines or two non-consecutive newlines. Thus we use the title to split the parsed PDF file, thus making sure that the second element contains the authors along with the afiliations. To separate the authors from the afiliations and the rest of the parsed file we implement a procedure similar to the one in the SAN 2014 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def san_2013_parser(poster_string):\n",
    "    #Topic\n",
    "    tema = poster_string.split('\\n')[0]\n",
    "    #Title\n",
    "    titulo = poster_string.split('\\n \\n')[1]\n",
    "    #If the title has a lenght greater than 1000 it means the variable contains more elements than just the title\n",
    "    if len(titulo) > 1000:\n",
    "        #It may be the spliting parameter so we try a different newline combination\n",
    "        titulo = poster_string.split('\\n\\n')[1]\n",
    "        #If this newline also fails (given by a huge length) it may separation between the title and the poster/session\n",
    "        if len(titulo) > 1000:\n",
    "            #To solve it, we split with the poster/session element and select the last element\n",
    "            #The poster/session element is the second element, splited by newlines, in the first element splited by two consecutive newlines\n",
    "            titulo = poster_string.split('\\n\\n')[0].split(poster_string.split('\\n\\n')[0].split('\\n')[1])[-1]\n",
    "    #Empty string to held the authors names\n",
    "    autores = ''\n",
    "    #Divide the string by the title    \n",
    "    potenciales_autores = poster_string.split(titulo)[1].split('\\n') #Element containing the authors, afiliations and poster text\n",
    "    #Index to start iterating in the first element of potenciales_autores\n",
    "    a_i = 0\n",
    "    #Continue until any word in the i element of potenciales_autores is in the list of filiaciones_keywords\n",
    "    #Any element beyond the one who fullfils these requirements doesn't contain any authors and is not of interest\n",
    "    while not any([p in filiaciones_keywords_2013 for p in potenciales_autores[a_i].split(' ')]):\n",
    "        print(a_i)\n",
    "        #Break the loop if it reaches a long underscore line which indicates the beginning of the poster text\n",
    "        if \"_\" in potenciales_autores[a_i]:\n",
    "            #If the loop reaches this conditional it means it failed to detect afiliations, thus display a warning\n",
    "            print('ERROR! No se detectaron filiaciones')\n",
    "            break\n",
    "        #Add authors to the autores list variable only if the i element has a length greater than 2\n",
    "        #This is done to avoid elements like ' ' to be added\n",
    "        elif len(potenciales_autores[a_i].strip()) > 2:\n",
    "            autores = autores + potenciales_autores[a_i]\n",
    "        else:\n",
    "        #Skip the element if above conditions are not met\n",
    "            pass\n",
    "        #Move on to the next element\n",
    "        a_i += 1 \n",
    "    #Split authors by commas to get one author per element in a list\n",
    "    autores = autores.split(',')\n",
    "    #Dictionary containing topic, title and authors from the poster\n",
    "    return pd.DataFrame(\n",
    "            {'tema': [tema], \n",
    "             'poster': [titulo], \n",
    "             'autor': [autores]}\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAN 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero el dataset que va a albergar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "san_2012_df = pd.DataFrame(columns= ['tema', 'autor', 'titulo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we follow the same procedure as in the SAN 2014 and 2013 by examining tokenized strings from elements eigth to eleven to come up by the affiliations keywords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = pd.Series()\n",
    "for i in range(42,249):\n",
    "    tmp_page = extract_text('SAN_2012.pdf', page_numbers=[i]).split('\\n')\n",
    "    try:\n",
    "        word_list = tk.tokenize(tmp_page[7] + tmp_page[8] + tmp_page[9] + tmp_page[10]) \n",
    "        res = res.append(pd.Series(word_list), True)\n",
    "    except:\n",
    "        continue\n",
    "res.value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filiaciones_keywords_2012 = ['conicet', 'uba', 'facultad', 'universidad', 'instituto', 'buenos aires', 'laboratorio', 'argentina', 'departamento', 'university', 'department']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes one argument, a string resulting from the parsing of a page from the pdf file.\n",
    "\n",
    "Poster theme is always the third element of the newline-separated document.\n",
    "\n",
    "To find the title we first iterate over each element of the newline-separated document until we find a reference to the poster number and session, the index of the following element is the beginning of the title. From there we move to the next element, adding it to the title string until a stopping criterion is met. The criterion is given by both the presence of a comma and the presence of at least one name from the MinCyT database. \n",
    "\n",
    "The index of the element which fulfills the stopping criterion is the beginning of the authors names. We move to the next element, adding it to the authors string until at least one afiliation keywords is found. \n",
    "\n",
    "If an at sign is detected when iterating over elements presumably containing authors names, and error is displayed and the authors names string is replaced by an error string.\n",
    "\n",
    "The function returns a pandas dataframe containing the theme, title and one author per row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def san_2012_parser(poster_string):\n",
    "    poster_split = poster_string.split('\\n')\n",
    "    tema = poster_split[2] #Theme\n",
    "    #Title\n",
    "    t_i = 0\n",
    "    try:\n",
    "        while 'poster number' not in poster_split[t_i].lower(): #Search elements by index until reference words are found\n",
    "            t_i += 1 #Go to the next element\n",
    "    except:\n",
    "        return\n",
    "    #t_i contains the poster and session reference\n",
    "    #The title starts at t_i + 1\n",
    "    t_i += 1\n",
    "    #Empty title string\n",
    "    titulo = ''\n",
    "    #Any element lacking a comma and a name is considered part of the title\n",
    "    while ',' not in poster_split[t_i] and not any([x in unidecode(poster_split[t_i].lower()) for x in nombres_personas.keys()]): \n",
    "        titulo = titulo + poster_split[t_i] #Add the element to the title\n",
    "        t_i += 1 #Go to the next element\n",
    "    #The authors start at the t_i + 1 element\n",
    "    a_i = t_i + 1\n",
    "    #Empty authors string \n",
    "    autores = ''\n",
    "    #If there is no match for the afilitations keywords then we assume the element contains authors names\n",
    "    while not any([p in poster_split[a_i].lower() for p in filiaciones_keywords_2012]):\n",
    "        #Add the element to the authors string\n",
    "        autores = autores + poster_split[a_i]\n",
    "        a_i += 1 #Move to the next element\n",
    "        #Check that the loop doesn't reach an element containing an 'at sign'\n",
    "        if '@' in poster_split[a_i]:\n",
    "            #If True\n",
    "            #Display an error message with the number of the page\n",
    "            print(f'ERROR! in {t_i}, page number {i}')\n",
    "            #Add the ERROR string to the authors strings to facilitate detection of failed pages\n",
    "            autores = autores + 'ERROR'\n",
    "            #Break the loop to avoid looping over other non-useful elements\n",
    "            break\n",
    "    return pd.DataFrame({'tema': tema, 'autor': autores.split(','), 'poster': titulo})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already now that, according to the function output, page 171 and 54 failed to identify authors. If we wish we could visually examine the output of _\"extract_text\"_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_text('SAN_2012.pdf', page_numbers=[54]).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_text('SAN_2012.pdf', page_numbers=[171]).split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are only two rows containing the word ERROR in the _\"autor\"_ column. For the page 54 poster, only the last author and the afiliations were detected and added as authors. Moreover the title was no captured. \n",
    "\n",
    "For the poster in page 171, the title was accurately captured but it failed to capture the authors name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "san_2012_df[san_2012_df['autor'].apply(lambda x: 'ERROR' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we eliminate these rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "san_2012_df = san_2012_df.drop([36,380])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add them manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "san_2012_df = pd.concat(\n",
    "[san_2012_df, pd.DataFrame({'tema': 'Cellular and Molecular Neurobiology', 'autor': ['Victor Danelon', \n",
    "                                                                        'Andrea B. Cragnolini', \n",
    "                                                                        'Daniel H. Masco'],\n",
    "              'titulo': \"An in vitro model of SE induces neuronal death and changes in the levels of TrkB, pTrkB and p75ntr receptors in a mixed culture of neurons and astrocytes\"})]\n",
    ")\n",
    "san_2012_df = pd.concat(\n",
    "[san_2012_df, pd.DataFrame({'tema': ['Computational Neuroscience'], 'autor': ['Juan Pablo Oliver'],\n",
    "                           'titulo': ['Active mapping of brain']})]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we reset the index and drop the redundant index columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "san_2012_df = san_2012_df.reset_index().drop(['level_0','index'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole process is done manually only once at the end of the document parsing for this pdf file in particular. Thus we encapsulate it within its own function to be applied at the end of the iteration process for the 2012 file only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def san_2012_fix(df_2012):\n",
    "    df_2012 = df_2012.drop([36,380])\n",
    "    df_2012 = pd.concat(\n",
    "        [df_2012, pd.DataFrame({'tema': 'Cellular and Molecular Neurobiology', 'autor': ['Victor Danelon', \n",
    "                                                                                'Andrea B. Cragnolini', \n",
    "                                                                                'Daniel H. Masco'],\n",
    "                      'titulo': \"An in vitro model of SE induces neuronal death and changes in the levels of TrkB, pTrkB and p75ntr receptors in a mixed culture of neurons and astrocytes\"})]\n",
    "        )\n",
    "    df_2012 = pd.concat(\n",
    "        [df_2012, pd.DataFrame({'tema': ['Computational Neuroscience'], 'autor': ['Juan Pablo Oliver'],\n",
    "                                   'titulo': ['Active mapping of brain']})]\n",
    "        )\n",
    "    df_2012 = df_2012.reset_index().drop(['level_0','index'], axis = 1)\n",
    "    return df_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SAN posters of the years 2019 to 2015 we generate an empty pandas data frame to hold the results from the parsing process and we create a dictionary containing:\n",
    "\n",
    "* The range of pages that hold posters\n",
    "* The parsing function for the given year \n",
    "* The pdf file for the given year\n",
    "\n",
    "The range of pages for each file is determined by manually inspecting the pdf file. \n",
    "\n",
    "Generate a dummy pandas dataframe from the data frame template.\n",
    "\n",
    "Next, iterate over each SAN year and then iterate over the range of poster containing pages from that year, select appropiate parsing function and file path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_template = pd.DataFrame(columns = ['autor', 'tema', 'poster'])\n",
    "\n",
    "san_posters = {\n",
    "    'san_2019': [range(2,257), san_2019_parser, '../SAN_Books/SAN_2019.pdf'],\n",
    "    'san_2018': [range(76,362), san_2018_parser, '../SAN_Books/SAN_2018.pdf'],\n",
    "    'san_2017': [range(48,212), san_2017_parser, '../SAN_Books/SAN_2017.pdf'],\n",
    "    'san_2015': [range(94,100), san_2015_parser, '../SAN_Books/SAN_2015.pdf'],\n",
    "    'san_2014': [range(45,192), san_2014_parser, '../SAN_Books/SAN_2014.pdf'],\n",
    "    'san_2013': [range(113,341), san_2013_parser, '../SAN_Books/SAN_2013.pdf'],\n",
    "    'san_2012': [range(42,249), san_2012_parser, '../SAN_Books/SAN_2012.pdf']\n",
    "}\n",
    "\n",
    "titulo_regex = re.compile(r'P(\\d)+.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most SAN posters files follow the same parsing logic by employing the _extract_text()_ function from the pdfminer library. However,the 2017 file requires a different parsing logic by combining two different methods from two different libraries thus requiring a conditional statement to allow flow control under the dictionary keys iteration. Likewise the 2012 file requires certain fixes after parsing. \n",
    "\n",
    "At the end of each iteration, the data frame generated from the pdf file is stored in a .csv file with a name matching the name of the dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def san_parser_wrapper():\n",
    "    tema_2014 = 'Cellular and Molecular Neurobiology'\n",
    "    personas = pd.read_csv('personas.csv', sep = ';', low_memory = False,\n",
    "                      encoding='UTF-8')\n",
    "    personas_na = personas[personas['nombre'].notna()]\n",
    "    nombres_personas = personas_na['nombre'].map(unidecode).str.lower().str.split(expand=True).stack().value_counts()\n",
    "    nombres_personas = nombres_personas[nombres_personas >= 150]\n",
    "    for i in san_posters.keys():\n",
    "        temporary_df = df_template.copy()\n",
    "        if i == 'san_2017':\n",
    "            #Open and read the corresponding pdf file\n",
    "            san_2017_pdf = open(san_posters[i][2], 'rb')\n",
    "            #Construct the reader object with the pdf file previously opened\n",
    "            san_2017_reader = PyPDF2.PdfFileReader(san_2017_pdf)\n",
    "            #Iterate as always over the range of pages\n",
    "            for p in san_posters[i][0]:\n",
    "                #First parsing method\n",
    "                parseo_1 = san_2017_reader.getPage(p).extractText()\n",
    "                #Second parsing method.\n",
    "                parseo_2 = extract_text(san_posters[i][2], page_numbers=[p])\n",
    "                temporary_df = pd.concat(\n",
    "                [\n",
    "                    temporary_df,\n",
    "                    san_posters[i][1](parseo_1, parseo_2)\n",
    "                ]\n",
    "                )\n",
    "        else:\n",
    "            for p in san_posters[i][0]:\n",
    "                temporary_df = pd.concat(\n",
    "                    [\n",
    "                    temporary_df, san_posters[i][1](extract_text(san_posters[i][2], page_numbers = [p]))\n",
    "                    ]\n",
    "                )\n",
    "        temporary_df = temporary_df.reset_index()\n",
    "        if i == 'san_2012':\n",
    "            temporary_df = san_2012_fix(temporary_df)\n",
    "        temporary_df.to_csv('../SAN_csv/' + i + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
