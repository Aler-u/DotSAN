{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from itertools import combinations\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to extract and clean the data from the website and are mainly focused on extracting the authors names and affiliations as well as establishing the relationship between the two. Unlike the pdf parsing functions, this one allow the clean extraction of affiliations and its relationship with every author. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a scrapy selector object and returns a list containing two lists, one for the poster authors and one for the poster affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraccion_autores(scrapy_sel):\n",
    "    nueva_lista_autores = []\n",
    "    lista_afiliaciones = []\n",
    "    autores_sel = scrapy_sel.xpath('//ul[@class=\"elementor-icon-list-items elementor-inline-items\"]')[1]\n",
    "    autores_lst = autores_sel.xpath('*//span/text()').extract()\n",
    "    nombre_autor = re.compile('\\D*[^¹²³⁴⁵⁶⁷⁸⁹𝄒1-9]')\n",
    "    num_afiliacion = re.compile('[¹²³⁴⁵⁶⁷⁸⁹]')\n",
    "    for a_i in range(len(autores_lst)):\n",
    "        match_nombre = nombre_autor.findall(autores_lst[a_i])\n",
    "        match_afil = num_afiliacion.findall(autores_lst[a_i])\n",
    "        if len(match_nombre) > 1:\n",
    "            print(f'Error en {match_nombre}')\n",
    "            break\n",
    "        nombre_sin_num = match_nombre[0].strip()\n",
    "        nombre_sin_coma = nombre_sin_num.replace(',','')\n",
    "        nombre_sin_w = nombre_sin_coma.strip()\n",
    "        nueva_lista_autores.append(nombre_sin_w)\n",
    "        lista_afiliaciones.append(match_afil)\n",
    "    return([nueva_lista_autores, lista_afiliaciones])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a scrapy Selector object, a list of authors and a list of affiliations and returns a pandas data frame object containing the poster authors and affiliations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraccion_afiliaciones(scrapy_sel, aut, afil):\n",
    "    afil_lst = scrapy_sel.xpath('//ul[@class=\"elementor-icon-list-items\"]//span/text()').extract()\n",
    "    afil_num = re.compile('\\d')\n",
    "    afil_name = re.compile('\\D+')\n",
    "    afil_dic = {}\n",
    "    for a_f in afil_lst:\n",
    "        afil_dic[int(afil_num.search(a_f).group())] = afil_name.search(a_f).group()\n",
    "    df_autor_y_afil = pd.DataFrame(\n",
    "    data = {'autor':aut, \n",
    "            'afiliacion': afil}\n",
    "            )\n",
    "    df_autor_y_afil['afiliacion'] = df_autor_y_afil['afiliacion'].apply(lambda x: afil_fun(x, afil_dic))\n",
    "    return(df_autor_y_afil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used by the 'extraccion_afiliaciones' function to get rid of superscript numbers in the affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afil_fun(row, afil_dic):\n",
    "\n",
    "    superscript_dic = {\n",
    "        chr(185): 1,\n",
    "        chr(178): 2,\n",
    "        chr(179): 3,\n",
    "        chr(8308): 4,\n",
    "        chr(8309): 5,\n",
    "        chr(8310): 6,\n",
    "        chr(8311): 7,\n",
    "        chr(8312): 8,\n",
    "        chr(8313): 9\n",
    "    }\n",
    "\n",
    "    if len(row) > 1:\n",
    "        res_lng = []\n",
    "        for af in row:\n",
    "            super_to_num = superscript_dic[af]\n",
    "            afil_ref = afil_dic[super_to_num]\n",
    "            res_lng.append(afil_ref)\n",
    "        return(res_lng)\n",
    "    else:\n",
    "        super_to_num = superscript_dic[row[0]]\n",
    "        res_sht = afil_dic[super_to_num]\n",
    "        return(res_sht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes the scraping itself and it is encapsulated within a function in the extraction.py file. \n",
    "\n",
    "First generate an empty pandas data frame to hold the results from the scraping process.  \n",
    "Second we instatiate the driver with selenium using firefox (which requires geckodriver). We had to use selenium to simulate a browser environment so that some of the required code from the website could run and thus showed the appropiate output to scrape.  \n",
    "Third we open the SAN 2020 website in the driver and extract the source code from the website to construct a scrapy Selector object from where we can extract the urls that contain all the posters.  \n",
    "\n",
    "Then we loop over all the poster urls while adding a conditional statement to check it is indeed a poster url before opening the url in the driver. After this conditional statement has been evaluated to True, the driver is redirected to the poster url for that iteration after which the source code for the new website is extracted and used to create a scrapy Selector object.  \n",
    "Still within the loop, from this object we can extract the poster title, topic, authors and affiliations. We deliver the extracted authors and affiliations to one of the helper functions that generates a data frame with both elements and then we add a column for the poster title and topic to said data frame as well as an indicator variable for the first author and the resulting data frame is appended to the final dataframe.\n",
    "\n",
    "Once the loop has finished the results are stored in a csv file. Then we extract the ids for the posters topics and the matching name for every poster topic. After cleaning the resulting strings we generate a dictionary with the poster topic id as key and the corresponding name as value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_autores = pd.DataFrame(columns = ['autor', 'afiliacion', 'titulo','tema', 'primer_autor'])\n",
    "\n",
    "san2020 = r'https://san2020.saneurociencias.org.ar/epostersbytopics/'\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(san2020)\n",
    "san_sel = scrapy.Selector(text=driver.page_source)\n",
    "todos_url = san_sel.xpath('//div[@class=\"elementor-row\"]//a/@href').extract()\n",
    "\n",
    "for u in todos_url:\n",
    "    if 'san2020.saneurociencias.org.ar/posters/' in u:    \n",
    "        driver.get(u)\n",
    "        poster_sel = scrapy.Selector(text = driver.page_source)\n",
    "        titulo_poster = poster_sel.xpath('//title/text()').extract()[0]\n",
    "        titulo_poster = titulo_poster.replace('– SAN2020','').strip()\n",
    "        tema_poster = poster_sel.xpath('//div[@class=\"elementor-element elementor-element-d3567ab elementor-column elementor-col-50 elementor-top-column\"]//h2[@class=\"elementor-heading-title elementor-size-default\"]/a[@rel=\"tag\"]/text()').extract()[0]\n",
    "        if tema_poster == poster_sel.xpath('//h2[@class=\"elementor-heading-title elementor-size-default\"]/a[@rel=\"tag\"]/text()').extract()[1]:\n",
    "            print('Extraccion de tema OK.')\n",
    "        else:\n",
    "            print(f'Inconsistencias en la extraccion del tema para poster:\\n{titulo_poster}')\n",
    "        autores_afil = extraccion_autores(poster_sel)\n",
    "        autores,afiliaciones = autores_afil[0], autores_afil[1]\n",
    "        aut_afil = extraccion_afiliaciones(poster_sel, autores, afiliaciones)\n",
    "        aut_afil['titulo'] = titulo_poster\n",
    "        aut_afil['tema'] = tema_poster\n",
    "        primer_autor = [False] * (len(autores) - 1)\n",
    "        primer_autor = primer_autor.insert(0, True)\n",
    "        aut_afil['primer_autor'] = primer_autor\n",
    "        df_autores = df_autores.append(\n",
    "            aut_afil\n",
    "        )\n",
    "\n",
    "df_autores.to_csv('../SAN_csv/SAN_2020_autores.csv')\n",
    "\n",
    "topics_id = san_sel.xpath('//div[@class=\"elementor-text-editor elementor-clearfix\"]/p/a/@href').extract()\n",
    "\n",
    "topics_name = san_sel.xpath('//div[@class=\"elementor-text-editor elementor-clearfix\"]/p/a/text()').extract()\n",
    "\n",
    "topics_id = [top.replace('#','') for top in topics_id]\n",
    "\n",
    "topic_dic = {}\n",
    "for i in range(len(topics_id)):\n",
    "    topic_dic[topics_id[i]] = topics_name[i]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ae8e237121b070a5de8d64c0638415fb86a17ae52eb5cef1445a5593409f524"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
