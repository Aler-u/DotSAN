{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This database will serve two purposes:\n",
    "\n",
    "* To enable record linkage in order to standarize authors names which may have mild variations in writing between different years or posters.\n",
    "* See which authors correspond to science and technology personel in order to exclude foreign authors, undergraduate students and others.\n",
    "\n",
    "According to the [data source](https://datos.gob.ar/dataset/mincyt-personal-ciencia-tecnologia), the following description of the dataset is given:\n",
    "\n",
    "> \"Conjunto de datos que describe mediante atributos tales como nombre, apellido, sexo, edad, a las personas físicas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = pd.read_csv('../CyT_Datasets/personas.csv', sep = ';', low_memory = False,\n",
    "                      encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine the dataset a bit closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 185618 entries, 0 to 185617\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   persona_id          185618 non-null  object \n",
      " 1   nombre              183411 non-null  object \n",
      " 2   apellido            182285 non-null  object \n",
      " 3   sexo_id             128420 non-null  float64\n",
      " 4   edad                128420 non-null  float64\n",
      " 5   cvar_ultimo_acceso  128330 non-null  object \n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "personas.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 185 thousand unique ids in the database which would supposedly translate to 185 thousand people. We want the names and the sex of each individual which means we only have 183 thousand names and 128 thousand non-null sex rows. Since we are limited by the lowest of these values which is the sex column, we only have data for 128 thousand individuals. \n",
    "\n",
    "Thus, we remove all the rows for which the sex columns is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas_limpia = personas[personas['sexo_id'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a new pandas series from the union of the firstname and lastname of every person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_personas = personas_limpia['nombre'] + ' ' + personas_limpia['apellido']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the names to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_personas = nombres_personas.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recode strange characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_personas = nombres_personas.apply(lambda x: unidecode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a last cleaning step using a regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_personas_clean = nombres_personas.apply(lambda x: re.sub(r'[^a-zA-Z]', r'', string = x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last variable contains the names withouth any whitespace and it will be the one we will use in our tf-idf implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use the data frame that contains data for all posters combined so we load the corresponding csv file altough we could use the output from the extraction.py file it would take an unnecessary long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posters_data = pd.read_csv('../SAN_csv/all_posters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only need one sample of each author name we remove duplicated names from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "autores_sin_dup = all_posters_data['autor'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'author' column contains the authors names as they were parsed from the posters but these strings must be normalized.\n",
    "\n",
    "First we convert to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_spam = autores_sin_dup.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we strip the trailing whitespaces, newlines, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_spam = name_spam.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any numbers and the character '°' which appeared frequently next to the numbers that indicated affiliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_spam = name_spam.apply(lambda x: re.sub(r'[\\d°]', r'', string = str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve coding issues like accents and other strange characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_spam = name_spam.apply(lambda x: unidecode(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we include only letters and remove any whitespace to use these strings in our tf-idf implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name_spam_clean = name_spam.apply(lambda x: re.sub(r'[^a-zA-Z]', r'', string = str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Linkage Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf and K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ngrams function separates the strings into trigrams by default, thus returning a list of ngrams for every string. It will be useful to use trigrams since many names may have spelling errors and other mistakes thus impeding correct matching when separating by entire words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n = 3):\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the tf-idf class, pass it the ngrams function we just made and create the tf-idf matrix from the clean names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase= False)\n",
    "tf_idf_matrix = vectorizer.fit_transform(nombres_personas_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiliaze the NearesNeighbors class using only one neighbor and fitting the tf-idf matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getNearestN function will get a query which will one of the strings we want to match and will return the index of the closest match from the clean names variable and the distance to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearestN(query):\n",
    "    queryTFIDF_ = vectorizer.transform(query)\n",
    "    distances, indices = nbrs.kneighbors(queryTFIDF_)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function to our messy authors names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.1 s, sys: 18 s, total: 1min\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%time dis, inds = getNearestN(name_spam_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Before procedding we can compare the timings of these method with the timing of more traditional fuzzy string matching by using the fuzzywuzzy library. \n",
    "\n",
    "We calculate the time it would take to search just one name so we can get an idea of how long would it take to calculate the whole dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.08 s, sys: 6.83 ms, total: 5.09 s\n",
      "Wall time: 5.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ignacio aiello', 95)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time process.extractBests(query= name_spam.iloc[0],choices= list(nombres_personas), limit = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe containing the query and the match as well as the distance calculated from the K-Nearest Neighbor algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_spam = list(name_spam) #need to convert back to a list\n",
    "matches = []\n",
    "for i,j in enumerate(inds):\n",
    "    temp = [round(dis[i][0],2), nombres_personas.values[j][0], name_spam[i], autores_sin_dup.values[i], personas_limpia['persona_id'].values[j][0], j[0]]\n",
    "    matches.append(temp)\n",
    "final_res = pd.DataFrame(matches, columns = ['score', 'match','messy', 'original', 'id', 'index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking matching validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although undoubtedly faster, the above algorithm may not be as accurate as others methods. But we can now use more traditional fuzzy string matching by using pairwise comparissons between the matched name from the K-Nearest Neighbors algorithm and the target name to create a criterion composed of multiple scores. \n",
    "\n",
    "For this implementation we employ 4 measures from the fuzzywuzzy library:\n",
    "\n",
    "* Partial Token Set Ratio: Should give 100 when a portion of the string is present in both cases. For example when they share a common lastname, independtely of the position. \n",
    "* Partial Ratio: Attempts to account for partial string matches.\n",
    "* Token Sort Ratio: Should give high scores independently of the words order when they don't differ.\n",
    "* WRatio: A balanced score considering many factors\n",
    "\n",
    "Furthermore, we take the average of the last 3 measures as a fifth metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_scorer(col1, col2):\n",
    "    partial_token_set = fuzz.partial_token_set_ratio(col1,col2)\n",
    "    partial_ratio = fuzz.partial_ratio(col1, col2)\n",
    "    token_sort = fuzz.token_sort_ratio(col1, col2)\n",
    "    wratio = fuzz.WRatio(col1, col2)\n",
    "    return [partial_token_set, partial_ratio, token_sort, wratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = []\n",
    "for indice, row in final_res.iterrows():\n",
    "    tmp_resultado = multiple_scorer(row['match'],row['messy'])\n",
    "    promedio = np.round(np.mean(tmp_resultado[1:]), 3)\n",
    "    tmp_resultado.append(promedio)\n",
    "    resultado.append(tmp_resultado)\n",
    "scores = pd.DataFrame(resultado, columns=['Partial Token Set', 'Partial Ratio', 'Token Sort', 'WRatio', 'Score Average'])\n",
    "names_matched = pd.merge(final_res, scores, left_index=True, right_index=True)\n",
    "names_matched.to_csv('matche_nombres.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
